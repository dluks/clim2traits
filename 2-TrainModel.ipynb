{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Train XGBoost Model\n",
    "\n",
    "Author: Daniel Lusk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacv\n",
    "\n",
    "from spacv.visualisation import plot_autocorrelation_ranges\n",
    "from TrainModelConfig import TrainModelConfig\n",
    "from utils.geodata import drop_XY_NAs\n",
    "from utils.visualize import plot_splits\n",
    "import utils.datasets as datasets\n",
    "from utils.datasets import DataCollection, Dataset, MLCollection, CollectionName\n",
    "from utils.dataset_tools import Unit, FileExt\n",
    "\n",
    "NOTEBOOK = True\n",
    "\n",
    "if NOTEBOOK:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "config = TrainModelConfig()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inat_orig = Dataset(\n",
    "    res=0.5,\n",
    "    unit=Unit.DEGREE,\n",
    "    collection_name=config.iNat_name,\n",
    "    transform=\"exp_ln\",\n",
    ")\n",
    "\n",
    "inat_dgvm = Dataset(\n",
    "    res=0.5,\n",
    "    unit=Unit.DEGREE,\n",
    "    collection_name=CollectionName.INAT_DGVM,\n",
    "    transform=\"exp_ln\",\n",
    ")\n",
    "\n",
    "inat_gbif = Dataset(\n",
    "    res=0.5,\n",
    "    unit=Unit.DEGREE,\n",
    "    collection_name=CollectionName.INAT_GBIF,\n",
    "    # filter_outliers=config.training_config.filter_y_outliers,\n",
    ")\n",
    "\n",
    "gbif = Dataset(\n",
    "    res=0.5,\n",
    "    unit=Unit.DEGREE,\n",
    "    collection_name=CollectionName.GBIF,\n",
    "    band=datasets.GBIFBand.MEAN,\n",
    "    file_ext=FileExt.GRID\n",
    ")\n",
    "\n",
    "splot = Dataset(\n",
    "    res=0.5,\n",
    "    unit=Unit.DEGREE,\n",
    "    collection_name=CollectionName.SPLOT,\n",
    "    band=datasets.GBIFBand.MEAN,\n",
    "    file_ext=FileExt.GRID\n",
    ")\n",
    "\n",
    "wc = Dataset(\n",
    "    res=0.5,\n",
    "    unit=Unit.DEGREE,\n",
    "    collection_name=config.WC_name,\n",
    ")\n",
    "\n",
    "modis = Dataset(\n",
    "    res=0.5,\n",
    "    unit=Unit.DEGREE,\n",
    "    collection_name=config.MODIS_name,\n",
    ")\n",
    "\n",
    "soil = Dataset(\n",
    "    res=0.5,\n",
    "    unit=Unit.DEGREE,\n",
    "    collection_name=config.soil_name,\n",
    ")\n",
    "\n",
    "vodca = Dataset(\n",
    "    res=0.5,\n",
    "    unit=Unit.DEGREE,\n",
    "    collection_name=CollectionName.VODCA,\n",
    "    file_ext=FileExt.NETCDF4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize the datasets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = DataCollection([wc, modis, soil, vodca])\n",
    "Y = DataCollection([inat_gbif])\n",
    "\n",
    "# Convert to MLCollection for training\n",
    "XY = MLCollection(X, Y)\n",
    "XY.drop_NAs(verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "To-Dos:\n",
    "\n",
    "1) ~~Create a data frame where you have all response variables and predictors.~~\n",
    "2) ~~Remove cells where you do not have a value for ANY predictor/response variable (you still may have NA for some columns then).~~\n",
    "3) ~~Train the models and do the evaluation~~\n",
    "4) Repeat step 3, but remove rows where you have at least one NA\n",
    "5) Compare accuracies of step 3 and 4 and see whatÂ´s best.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate autocorrelation range of predictors and generate spatial folds for spatial cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.SAVE_AUTOCORRELATION_RANGES:\n",
    "    coords = XY.df[\"geometry\"]\n",
    "    data = XY.df[XY.X.cols]\n",
    "\n",
    "    _, _, ranges = plot_autocorrelation_ranges(\n",
    "        coords, data, config.LAGS, config.BW, distance_metric=\"haversine\", workers=10\n",
    "    )\n",
    "\n",
    "    np.save(\"ranges.npy\", np.asarray(ranges))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore splits for a single response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.EXPLORE_SPLITS:\n",
    "    y_col = \"iNat_Stem.conduit.density_05deg_ln\"\n",
    "    sample_Xy = XY.df[[\"geometry\", *XY.X.cols, y_col]]\n",
    "\n",
    "    # Drop full-NAs\n",
    "    sample_Xy, sample_X_cols, sample_y_col = drop_XY_NAs(\n",
    "        sample_Xy, XY.X.cols, y_col, True\n",
    "    )\n",
    "\n",
    "    # Sample X data on which split dissimilarity will be measured\n",
    "    sample_data = sample_Xy[sample_X_cols]\n",
    "    sample_locs = sample_Xy[\"geometry\"]\n",
    "\n",
    "    # Grid settings\n",
    "    tiles_x = int(np.round(360 / config.AUTOCORRELATION_RANGE))\n",
    "    tiles_y = int(np.round(180 / config.AUTOCORRELATION_RANGE))\n",
    "\n",
    "    # Spatial blocking\n",
    "    hblock = spacv.HBLOCK(\n",
    "        tiles_x,\n",
    "        tiles_y,\n",
    "        shape=\"hex\",\n",
    "        method=\"optimized_random\",\n",
    "        buffer_radius=0.01,\n",
    "        n_groups=10,\n",
    "        data=sample_data,\n",
    "        n_sims=50,\n",
    "        distance_metric=\"haversine\",\n",
    "        random_state=config.RNG_STATE,\n",
    "    )\n",
    "\n",
    "    # Plot splits\n",
    "    print(f\"Tile size: {config.AUTOCORRELATION_RANGE:.2f} degrees\")\n",
    "    plot_splits(hblock, sample_locs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models for each response variable (better to use `2-TrainModel.py` script now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### TRAINING  ####################\n",
    "if config.TRAIN_MODE:\n",
    "    config = TrainModelConfig()\n",
    "    XY.train_Y_models(config.training_config, resume=True)\n",
    "##################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tighten hyperparameter ranges based on results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"results/training_results.csv\")\n",
    "params = results[\"Best parameters\"]\n",
    "params = [ast.literal_eval(p) for p in params if p is not np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# create a list of parameter names\n",
    "param_names = list(params[0].keys())\n",
    "\n",
    "# calculate the number of rows and columns needed for the subplots\n",
    "n_rows = len(param_names) // 3 + len(param_names) % 3\n",
    "n_cols = 3\n",
    "\n",
    "# create a figure with 3 columns of subplots\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(16, 6*n_rows))\n",
    "\n",
    "# create a box and whisker plot for each parameter\n",
    "for i, name in enumerate(param_names):\n",
    "    # calculate the row and column index for this subplot\n",
    "    row = i // n_cols\n",
    "    col = i % n_cols\n",
    "\n",
    "    # create a list of parameter values for this parameter\n",
    "    param_values = [d[name] for d in params]\n",
    "\n",
    "    # calculate the quartile values\n",
    "    q1, median, q3 = np.percentile(param_values, [25, 50, 75])\n",
    "\n",
    "    # create a box and whisker plot for this parameter\n",
    "    axs[row, col].boxplot(param_values)\n",
    "\n",
    "    # set informative x-axis labels and title\n",
    "    axs[row, col].set_xticklabels([name])\n",
    "    axs[row, col].set_xlabel(\"Parameter\")\n",
    "    axs[row, col].set_ylabel(\"Value\")\n",
    "    axs[row, col].set_title(f\"Distribution of {name}\")\n",
    "\n",
    "    # add text with quartile values\n",
    "    axs[row, col].text(0.95, 0.95, f\"Q1: {q1:.2f}\\nMed: {median:.2f}\\nQ3: {q3:.2f}\",\n",
    "                        transform=axs[row, col].transAxes, ha=\"right\", va=\"top\")\n",
    "\n",
    "    # add sample density along right axis\n",
    "    kde = gaussian_kde(param_values)\n",
    "    x_vals = np.linspace(min(param_values), max(param_values), 100)\n",
    "    axs[row, col].twinx().plot(kde(x_vals), x_vals, color='red')\n",
    "    axs[row, col].set_ylim(min(param_values), max(param_values))\n",
    "\n",
    "# remove any unused subplots\n",
    "for i in range(len(param_names), n_rows*n_cols):\n",
    "    row = i // n_cols\n",
    "    col = i % n_cols\n",
    "    fig.delaxes(axs[row, col])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.training import train_model_full\n",
    "from utils.training import block_cv_splits, optimize_params\n",
    "from utils.datasets import Dataset, DataCollection, MLCollection\n",
    "from utils.geodata import drop_XY_NAs\n",
    "from utils.dataset_tools import Unit\n",
    "from TrainModelConfig import TrainModelConfig\n",
    "\n",
    "\n",
    "config = TrainModelConfig(debug=True)\n",
    "\n",
    "if config.DEBUG:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    \n",
    "    res = 0.5\n",
    "    y_transform = \"exp_ln\"\n",
    "\n",
    "    inat_orig = Dataset(\n",
    "        res=res,\n",
    "        unit=Unit.DEGREE,\n",
    "        collection_name=config.iNat_name,\n",
    "        transform=y_transform,\n",
    "    )\n",
    "\n",
    "    wc = Dataset(\n",
    "        res=res,\n",
    "        unit=Unit.DEGREE,\n",
    "        collection_name=config.WC_name,\n",
    "    )\n",
    "\n",
    "    X = DataCollection([wc])\n",
    "    Y = DataCollection([inat_orig])\n",
    "\n",
    "    print(\"\\nPreparing data...\")\n",
    "    print(\"X:\")\n",
    "    for dataset in X.datasets:\n",
    "        print(\"    \", dataset.collection_name.short)\n",
    "\n",
    "    print(\"Y:\")\n",
    "    for dataset in Y.datasets:\n",
    "        print(\"    \", dataset.collection_name.short)\n",
    "\n",
    "    # Convert to MLCollection for training\n",
    "    XY = MLCollection(X, Y)\n",
    "    XY.drop_NAs(verbose=1)\n",
    "\n",
    "    y_col = XY.Y.cols[0]\n",
    "\n",
    "    Xy = XY.df[[\"geometry\", *XY.X.cols, y_col]]\n",
    "    Xy, X_cols, y_cols = drop_XY_NAs(Xy, XY.X.cols, y_col, True)\n",
    "\n",
    "    X = Xy[X_cols].to_numpy()\n",
    "    y = Xy[y_col].to_numpy()\n",
    "    coords = Xy[\"geometry\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test, coords_train, coords_test = train_test_split(\n",
    "        X, y, coords, test_size=0.2, random_state=config.training_config.random_state\n",
    "    )\n",
    "\n",
    "    cv = block_cv_splits(\n",
    "        X=X_train,\n",
    "        coords=coords_train,\n",
    "        grid_size=config.training_config.cv_grid_size,\n",
    "        n_groups=10,\n",
    "        random_state=config.training_config.random_state,\n",
    "        verbose=1,\n",
    "    )\n",
    "    id = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    save_dir = config.training_config.results_dir / \"ray-results\" / id\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    reg = optimize_params(\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        col_name=y_col,\n",
    "        cv=cv,\n",
    "        save_dir=save_dir,\n",
    "        n_trials=200,\n",
    "        random_state=config.training_config.random_state,\n",
    "        max_iters=13,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    model, r2 = train_model_full(\n",
    "        model_params=reg.best_params_,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        verbose=1,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
